{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing modules and setting up matplotlib to render inside the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the NeuralNetwork class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, relu_alpha=0.01):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "\n",
    "        Args:\n",
    "            layers: Number of neurons for each layer.\n",
    "            relu_alpha: Parameter of leaky ReLU (set to 0 for default ReLU behaviour).\n",
    "\n",
    "        Returns:\n",
    "            New NerualNetwork instance.\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.relu_alpha = relu_alpha\n",
    "        self.W = [None] * (len(layers) - 1)\n",
    "        self.dW = [None] * (len(layers) - 1)\n",
    "        self.b = [None] * (len(layers) - 1)\n",
    "        self.db = [None] * (len(layers) - 1)\n",
    "        self.z = [None] * len(layers)\n",
    "        self.a = [None] * len(layers)\n",
    "        \n",
    "        for i in range(1, len(layers)):\n",
    "            # Using He initialization. Link at the sources section.\n",
    "            self.W[i - 1] = np.random.randn(layers[i - 1], layers[i]) * np.sqrt(2 / layers[i - 1])\n",
    "            self.b[i - 1] = np.random.randn(1, layers[i]) * np.sqrt(2 / layers[i - 1])\n",
    "            \n",
    "\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Runs network.\n",
    "\n",
    "        Args:\n",
    "            X: Input with shape==(N, D) (N: number of input rows, D: input dimension).\n",
    "\n",
    "        Returns:\n",
    "            Softmax probabilities of classes with shape==(N, K) (K: number of classes).\n",
    "        \"\"\"\n",
    "        self.a[0] = X\n",
    "        \n",
    "        for i in range(len(self.W)):\n",
    "            self.z[i + 1] = self.a[i].dot(self.W[i]) + self.b[i]\n",
    "            \n",
    "            if i == len(self.W) - 1:\n",
    "                self.probs = np.e**self.z[i + 1]\n",
    "                self.probs = self.probs / np.sum(self.probs, axis=1, keepdims=True)\n",
    "                return self.probs\n",
    "            \n",
    "            self.a[i + 1] = self.z[i + 1].copy()\n",
    "            self.a[i + 1][self.a[i + 1] < 0] *= self.relu_alpha\n",
    "    \n",
    "    \n",
    "    def backward(self, y):\n",
    "        \"\"\"\n",
    "        Computes gradient based on previous forward run.\n",
    "\n",
    "        Args:\n",
    "            y: One-hot encoded target output labels.\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "        \"\"\"\n",
    "        dz = (self.probs - y) / len(y)\n",
    "        \n",
    "        for i in reversed(range(len(self.W))):\n",
    "            self.dW[i] = self.a[i].T.dot(dz)\n",
    "            self.db[i] = dz.sum(axis=0, keepdims=True)\n",
    "            \n",
    "            if i == 0:\n",
    "                break\n",
    "\n",
    "            da = dz.dot(self.W[i].T)\n",
    "            dz = da\n",
    "            dz[self.z[i] < 0] *= self.relu_alpha\n",
    "        \n",
    "        \n",
    "    def step(self, lr):\n",
    "        \"\"\"\n",
    "        Performs one step of gradient descent.\n",
    "\n",
    "        Args:\n",
    "            lr: Learning rate.\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "        \"\"\"\n",
    "        for i in range(len(self.W)):\n",
    "            self.W[i] -= self.dW[i] * lr\n",
    "            self.b[i] -= self.db[i] * lr\n",
    "\n",
    "\n",
    "    def check_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        Checks gradient by comparing it to numerical gradient.\n",
    "\n",
    "        Args:\n",
    "            X: Input with shape==(N, D) (N: number of input rows, D: input dimension).\n",
    "            y: One-hot encoded target output labels.\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "        \"\"\"\n",
    "        h = 10**-6\n",
    "        threshold = 10**-6\n",
    "        \n",
    "        for k in range(len(self.W)):\n",
    "            for i in range(self.W[k].shape[0]):\n",
    "                for j in range(self.W[k].shape[1]):\n",
    "                    old = self.W[k][i, j]\n",
    "                    self.W[k][i, j] = old - h\n",
    "                    probs = self.forward(X)\n",
    "                    loss1 = -np.log((probs * y).sum(axis=1)).mean()\n",
    "                    \n",
    "                    self.W[k][i, j] = old + h\n",
    "                    probs = self.forward(X)\n",
    "                    loss2 = -np.log((probs * y).sum(axis=1)).mean()\n",
    "                    \n",
    "                    grad = (loss2 - loss1) / (2 * h)\n",
    "                    self.W[k][i, j] = old\n",
    "                    \n",
    "                    if np.abs(grad - self.dW[k][i, j]) > threshold:\n",
    "                        raise ValueError('Incorrect gradient.')\n",
    "            \n",
    "            for j in range(self.b[k].shape[1]):\n",
    "                old = self.b[k][0, j]\n",
    "                self.b[k][0, j] = old - h\n",
    "                probs = self.forward(X)\n",
    "                loss1 = -np.log((probs * y).sum(axis=1)).mean()\n",
    "\n",
    "                self.b[k][0, j] = old + h\n",
    "                probs = self.forward(X)\n",
    "                loss2 = -np.log((probs * y).sum(axis=1)).mean()\n",
    "\n",
    "                grad = (loss2 - loss1) / (2 * h)\n",
    "                self.b[k][0, j] = old\n",
    "                \n",
    "                if np.abs(grad - self.db[k][0, j]) > threshold:\n",
    "                    raise ValueError('Incorrect gradient.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to generate sample data\n",
    "We are generating a 2D data set. The data points lie on concentric circles so they are not linearly separable. Because of this a neural network with at least one hidden layer is needed to classify them correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(D, K, KN):\n",
    "    \"\"\"\n",
    "    Generates sample data.\n",
    "    \n",
    "    Args:\n",
    "        D: Input dimension.\n",
    "        K: Number of classes.\n",
    "        KN: Number of instances per class.\n",
    "        \n",
    "    Returns:\n",
    "        Sample X with shape==(K*KN, D) and y with shape==(K*KN, K).\n",
    "    \"\"\"\n",
    "    N = K * KN\n",
    "    X = np.zeros((N, D))\n",
    "    y = np.zeros((N, K))\n",
    "\n",
    "    for i in range(K):\n",
    "        angle = np.random.uniform(0, 2 * np.pi, KN)\n",
    "        noise = np.random.randn(KN) * 0.2\n",
    "        X[i*KN:(i+1)*KN, 0] = np.cos(angle) * (i + 0.4 + noise)\n",
    "        X[i*KN:(i+1)*KN, 1] = np.sin(angle) * (i + 0.4 + noise)\n",
    "        y[i*KN:(i+1)*KN, i] = 1\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the training data and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "gradient_checking_enabled = False\n",
    "D = 2\n",
    "K = 3\n",
    "KN = 100\n",
    "\n",
    "tr_X, tr_y = generate_data(D, K, KN)\n",
    "plt.axes().set_aspect('equal', 'box')\n",
    "plt.plot(tr_X[:KN, 0], tr_X[:KN, 1], 'ro')\n",
    "plt.plot(tr_X[KN:2*KN, 0], tr_X[KN:2*KN, 1], 'go')\n",
    "plt.plot(tr_X[2*KN:, 0], tr_X[2*KN:, 1], 'bo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create neural network and run the training process\n",
    "One hidden layer consisting of 16 neurons should be adequate to classify the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetwork([D, 16, K])\n",
    "\n",
    "for i in range(201):\n",
    "    probs = net.forward(tr_X)\n",
    "    loss = -np.log((probs * tr_y).sum(axis=1)).mean()\n",
    "    net.backward(tr_y)\n",
    "    \n",
    "    if gradient_checking_enabled:\n",
    "        net.check_gradient(tr_X, tr_y)\n",
    "    \n",
    "    net.step(0.2)\n",
    "    print(f'Epoch #{i}, loss: {loss}') if i % 10 == 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X, test_y = generate_data(D, K, KN)\n",
    "probs = net.forward(test_X)\n",
    "preds = probs.argmax(axis=1)\n",
    "correct = (probs.argmax(axis=1) == test_y.argmax(axis=1)).sum()\n",
    "print(f'Test set accuracy: {100 * correct / (K * KN):.2f}%')\n",
    "\n",
    "coord_range = np.linspace(-3, 3, 100)\n",
    "img = []\n",
    "\n",
    "for c2, c1 in itertools.product(coord_range, coord_range):\n",
    "        probs = net.forward(np.array([[c1, c2]]))\n",
    "        predicted_class = np.argmax(probs)\n",
    "        img.append(predicted_class)\n",
    "\n",
    "img = np.array(img).reshape(100, 100)        \n",
    "colors = ['red', 'green', 'blue']\n",
    "\n",
    "plt.plot(test_X[:KN, 0], test_X[:KN, 1], 'ro')\n",
    "plt.plot(test_X[KN:2*KN, 0], test_X[KN:2*KN, 1], 'go')\n",
    "plt.plot(test_X[2*KN:, 0], test_X[2*KN:, 1], 'bo')\n",
    "plt.imshow(img, cmap=LinearSegmentedColormap.from_list('rgb', colors, N=K), alpha=0.5,\n",
    "           origin='lower', extent=[-3, 3, -3, 3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources used:  \n",
    "https://math.stackexchange.com/questions/945871/derivative-of-softmax-loss-function  \n",
    "https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
