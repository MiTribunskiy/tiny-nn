{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sources used:\n",
    "# https://math.stackexchange.com/questions/945871/derivative-of-softmax-loss-function\n",
    "# https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, relu_alpha=0.01, std=0.1):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "\n",
    "        Args:\n",
    "            layers: Number of neurons for each layer.\n",
    "            relu_alpha: Parameter of leaky ReLU (set to 0 for default ReLU behavoiur).\n",
    "\n",
    "        Returns:\n",
    "            New NerualNetwork instance.\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.relu_alpha = relu_alpha\n",
    "        self.W = [None] * (len(layers) - 1)\n",
    "        self.dW = [None] * (len(layers) - 1)\n",
    "        self.b = [None] * (len(layers) - 1)\n",
    "        self.db = [None] * (len(layers) - 1)\n",
    "        self.z = [None] * len(layers)\n",
    "        self.a = [None] * len(layers)\n",
    "        \n",
    "        for i in range(1, len(layers)):\n",
    "            self.W[i - 1] = np.random.randn(layers[i - 1], layers[i]) * std\n",
    "            self.b[i - 1] = np.random.randn(1, layers[i]) * std\n",
    "\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Runs network.\n",
    "\n",
    "        Args:\n",
    "            X: Input with shape==(N, D) (N: number of input rows, D: input dimension).\n",
    "\n",
    "        Returns:\n",
    "            Softmax probabilities of classes.\n",
    "        \"\"\"\n",
    "        self.a[0] = X\n",
    "        \n",
    "        for i in range(len(self.W)):\n",
    "            self.z[i + 1] = self.a[i].dot(self.W[i]) + self.b[i]\n",
    "            \n",
    "            if i == len(self.W) - 1:\n",
    "                self.probs = np.e**self.z[i + 1]\n",
    "                self.probs = self.probs / np.sum(self.probs, axis=1, keepdims=True)\n",
    "                return self.probs\n",
    "            \n",
    "            self.a[i + 1] = self.z[i + 1].copy()\n",
    "            self.a[i + 1][self.a[i + 1] < 0] *= self.relu_alpha\n",
    "    \n",
    "    \n",
    "    def backward(self, y):\n",
    "        \"\"\"\n",
    "        Computes gradient based on previous forward run.\n",
    "\n",
    "        Args:\n",
    "            y: One-hot encoded target output labels.\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "        \"\"\"\n",
    "        dz = (self.probs - y) / len(y)\n",
    "        \n",
    "        for i in reversed(range(len(self.W))):\n",
    "            self.dW[i] = self.a[i].T.dot(dz)\n",
    "            self.db[i] = dz.sum(axis=0, keepdims=True)\n",
    "            \n",
    "            if i == 0:\n",
    "                break\n",
    "\n",
    "            da = dz.dot(self.W[i].T)\n",
    "            dz = da\n",
    "            dz[self.z[i] < 0] *= self.relu_alpha\n",
    "        \n",
    "        \n",
    "    def step(self, lr):\n",
    "        \"\"\"\n",
    "        Performs one step of gradient descent.\n",
    "\n",
    "        Args:\n",
    "            lr: Learning rate.\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "        \"\"\"\n",
    "        for i in range(len(self.W)):\n",
    "            self.W[i] -= self.dW[i] * lr\n",
    "            self.b[i] -= self.db[i] * lr\n",
    "\n",
    "\n",
    "    def check_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        Checks gradient by comparing it to numerical gradient.\n",
    "\n",
    "        Args:\n",
    "            X: Input with shape==(N, D) (N: number of input rows, D: input dimension).\n",
    "            y: One-hot encoded target output labels.\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "        \"\"\"\n",
    "        h = 10**-6\n",
    "        threshold = 10**-6\n",
    "        \n",
    "        for k in range(len(self.W)):\n",
    "            for i in range(self.W[k].shape[0]):\n",
    "                for j in range(self.W[k].shape[1]):\n",
    "                    old = self.W[k][i, j]\n",
    "                    self.W[k][i, j] = old - h\n",
    "                    probs = self.forward(X)\n",
    "                    loss1 = -np.log((probs * y).sum(axis=1)).mean()\n",
    "                    \n",
    "                    self.W[k][i, j] = old + h\n",
    "                    probs = self.forward(X)\n",
    "                    loss2 = -np.log((probs * y).sum(axis=1)).mean()\n",
    "                    \n",
    "                    grad = (loss2 - loss1) / (2 * h)\n",
    "                    print(f'dW {grad - self.dW[k][i, j]}')\n",
    "                    self.W[k][i, j] = old\n",
    "            \n",
    "            for j in range(self.b[k].shape[1]):\n",
    "                old = self.b[k][0, j]\n",
    "                self.b[k][0, j] = old - h\n",
    "                probs = self.forward(X)\n",
    "                loss1 = -np.log((probs * y).sum(axis=1)).mean()\n",
    "\n",
    "                self.b[k][0, j] = old + h\n",
    "                probs = self.forward(X)\n",
    "                loss2 = -np.log((probs * y).sum(axis=1)).mean()\n",
    "\n",
    "                grad = (loss2 - loss1) / (2 * h)\n",
    "                print(f'db {grad - self.db[k][0, j]}')\n",
    "                self.b[k][0, j] = old\n",
    "\n",
    "\n",
    "D = 2 # Input dimension\n",
    "K = 3 # Number of classes\n",
    "KN = 100 # Number of instances per class\n",
    "N = K * KN # Total number of instances\n",
    "\n",
    "X = np.zeros((N, D))\n",
    "y = np.zeros((N, K))\n",
    "\n",
    "for i in range(K):\n",
    "    angle = np.random.uniform(0, 2 * np.pi, (KN,))\n",
    "    noise = np.random.randn(KN) * 0.2\n",
    "    X[i*KN:(i+1)*KN, 0] = np.cos(angle) * (i + 0.4 + noise)\n",
    "    X[i*KN:(i+1)*KN, 1] = np.sin(angle) * (i + 0.4 + noise)\n",
    "    y[i*KN:(i+1)*KN, i] = 1\n",
    "    \n",
    "plt.axes().set_aspect('equal', 'datalim')\n",
    "plt.plot(X[:KN, 0], X[:KN, 1], 'ro')\n",
    "plt.plot(X[KN:2*KN, 0], X[KN:2*KN, 1], 'go')\n",
    "plt.plot(X[2*KN:, 0], X[2*KN:, 1], 'bo')\n",
    "\n",
    "nn = NeuralNetwork([2, 8, 3])\n",
    "\n",
    "for epoch in range(100):\n",
    "    probs = nn.forward(X)\n",
    "    loss = -np.log((probs * y).sum(axis=1)).mean()\n",
    "    print(f'loss: {loss}')\n",
    "    nn.backward(y)\n",
    "#     nn.check_gradient(X, y)\n",
    "    nn.step(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
